
<p>
1 马氏距离的理解
</p>
<p>
\triangle\text{＝}\sqrt{\left(x-&mu;\right)<sup>T</sup>&sum;<sup>-1</sup>(x-&mu;)} 这个是多维的形式
</p>
<p>
其本质形式将会是这样的：
</p>
<p>
&Delta;=w<sup>T</sup>d=w<sub>1</sub>d<sub>1</sub>+w<sub>2</sub>d<sub>2</sub>+&hellip;+w<sub>n</sub>d<sub>n</sub>
</p>
<p>
也就是各种距离的线性加权
</p>
<p>
再看看精度矩阵&sum;<sup>-1</sup> ，也就是协方差矩阵的逆，其实内面的各项就是上面的w1,w2,&hellip;,wn
</p>
<p>
回想协方差的含意，协方差是指线性相关的程度，这意味着，线性加权的这个权重，是按各种距离之前的线性相关程度定的。
</p>
<p>
如此一来就可以这样理解马氏距离了：把多种距离组合起来用，这些距离之间是有冗余的，也就是他们是线性相关的。冗余信息越大，给他加的权就越小。
</p>
<p>
举个例子，图像特征提取，颜色用直方图，RGB,HSV,LAB抽了三种颜色特征。纹理用Gabor小波，Habor不记得是不是这种形式了，取了四种纹理特征，再取个SIFT特征，共7种特征，用马氏距离。由于颜色特征之间的三种都是颜色，是有冗余信息的，于是权值就变小。颜色和纹理之间冗余信息就应该少一些，权值应该会大一些。
</p>
<p>
2 信息量，熵，KL散度，互信息
</p>
<p>
用一件事情发生的可能性来衡量这件事件的信息量。举个例子：人分为男人和女人，这条消息是绝对的，也就是确定性为1，所以一点信息量也没有。再举一个，学校明天给每个人发1000块钱。这个事件发生的可能性很低很低，所以这个事件的信息量很大。
</p>
<p>
按这种思想，定义信息量的公式
</p>
<p>
h(x)=-log<sub>2</sub>p(x)
</p>
<p>
一个随机变量可能取多个值，每个值有一定的概率。这个随机变量的平均信息量，就是他的熵
</p>
<p>
H(x)=-&sum;<sub>x</sub>p(x)log<sub>2</sub>p(x)
</p>
<p>
熵能衡量分布情况。分布得越松散，熵越大。于是二个分布的差异情况，就可以用熵的差值来度量，这就引出了KL散度
</p>
<p>
KL散度是来度量二个分布的差异情况的
</p>
<p>
KL(p||q)=-&int; p(x)ln q(x)dx-(-&int; p(x)ln p(x)dx)=-&int; p(x)ln\left\{ \frac{q(x)}{p(x)}\right\} dx
</p>
<p>
类似的还有互信息。我们知道如果二个变量独立，刚
</p>
<p>
p(x,y)=p(x)p(y)
</p>
<p>
只要x和y不是完全独立的，则p(x,y)不等于p(x)p(y)。于是用
</p>
<p>
p(x,y)和p(x)p(y)之前的KL散度来度量x与y的相关程度，这就叫互信息
</p>
<p>
I[x,y]=KL(p(x,y)||p(x)p(y))=-\iint p(x,y)ln(\frac{p(x)p(y)}{p(x,y)})dxdy
</p>
<p>
互信息可以度量二个变量的相关程度。协方差也可以，但协方差是度量线性相关程度的，这里互信息限制线性相关
</p>
<p>
3 非参估计的核方法
</p>
<p>
直方图可以看作分布函数
</p>
<p>
落在某一小块的概率p<sub>i</sub>=\frac{n<sub>i</sub>}{N&Delta;<sub>i</sub>} 其中&Delta;<sub>i</sub> 等于bin的宽度
</p>
<p>
p(x)=\frac{K}{NV}
</p>
<p>
固定K而V的值由数据决定：KNN方法
</p>
<p>
固定V而K的值由数据决定：核方法
</p>
<p>
k(\frac{x-x<sub>n</sub>}{h})=\begin{cases}
1 &amp; |x-x<sub>n</sub>|&lt;\frac{1}{2}<br/>
0 &amp; otherwise
\end{cases}
</p>
<p>
x是区域h的中点，这就是一个核函数
</p>
<p>
K=&sum;<sub>n=1</sub><sup>N</sup>k(\frac{x-x<sub>n</sub>}{h})
</p>
<p>
于是得到 p(x)=\frac{1}{N}&sum;<sub>n=1</sub><sup>N</sup>\frac{1}{h<sup>D</sup>}k(\frac{x-x<sub>n</sub>}{h})
</p>
<p>
高斯核
</p>
<p>
p(x)=\frac{1}{N}&sum;<sub>n=1</sub><sup>N</sup>\frac{1}{\left(2&pi; h<sup>2</sup>\right)<sup>2</sup>}exp\left\{ \frac{\parallel x-x<sub>n</sub>\parallel<sup>2</sup>}{h}\right\}
</p>
<p>
这里可以看到，核方法最终也成了一个形式，要确定一个参数h。这个形式和参数模型的形式很像高斯核得到的形式与利用高斯分布的参数模型。 看上去是参数模型与核方法的殊途同归，但是要注意区别：核方法是非参模型。前面的参数模型是要搞定一个分布，假设分布的形式定了，参数未知，要把参数搞定。而核方法也是搞定这个分布，不过不对分布的形式做假设，而是用样本的直方图来拟合分布，用核函数做数学变换。
</p>
<p>
核函数必须满足：
</p>
<p>
k(u)&gt;0 &int; k(u)du=1
</p>
<p>
4 各章节的内容疏理
</p>
<p>
第一章的主要讲了四个内容：
</p>
<p>
最小二乘法做线性拟合。用这个引出讨论内容，整个回归问题都是使用的线性拟合做例子
</p>
<p>
概率论。重点要掌握三种模型求解的方法：最大似然，最大后验，直接求解
</p>
<p>
决策论。二种决策方式：最小错误率和最小损失
</p>
<p>
信息论。信息量，熵，KL距离，互信息
</p>
<p>
第二章的主线：
</p>
<p>
整章都是讲各种分布，但实际上是有规律的。讲了共轭的概念
</p>
<p>
从最简单的分布伯努力分布开始，然后伯努利分布的极限形式是二项分布。然后讲二项分布的共轭分布&beta; 分布。再到多项式分布，以及多项式分布的共轭分布dirichlet分布。接着是高斯分布，高斯分布的共轭仍然是高斯，所以把高斯分布的形式泛化了一个，泛化后的形式对应的共轭分布是&gamma; 分布。由于&gamma; 分布的二个参数决定形状，分布的形状差别很大，所以把参数限定一下。于是从限定参数后的&gamma; 分布引出了学生T分布。
</p>
<p>
二项分布，高斯分布等等，这些就是某个分布的特例，于是讲了这个更泛化的形式：指数family。最后提了一个周期性分布
</p>
<p>
第三章主线：
</p>
<p>
先是用最大释然方法，然后推导得出最大释然法与最小二乘法是等价的。
</p>
<p>
为了解决过拟合的问题，引入带惩罚项的错误函数。但还是惩罚项的系统不好定，相当于将一个不好解的问题转移到别一个问题，但是还是没有解决问题。
</p>
<p>
然后插入讲了一个偏差方差分解。讲偏差方差分解是为了从另一个角度看模型复杂度。
</p>
<p>
改用贝叶斯方法，贝叶斯方法可以解决过拟合。贝叶斯推出来的形式也是与最小二乘法差不多的，但它解决了参数怎么取的问题。
</p>
<p>
实际上我们并不关心参数w，那么预测分布是一个更贝叶斯化的方法
</p>
<p>
又插入讲了一个等价核。把参数最大后验代进去，得到y\left(x,m<sub>n</sub>\right)=\text{\ensuremath{&sum;}}<sub>n=1</sub><sup>N</sup>k\left(x,x<sub>n</sub>\right)t<sub>n</sub> 引出了等价核的概念
</p>
<p>
以贝叶斯观点看模型选择问题结束本章
</p>
<p>
5 偏差方差分解
</p>
<p>
h(\mathbf{x})=E\left[t|\mathbf{x}\right]=&int; tp(t|\mathbf{x})dt
</p>
<p>
回归函数是t的期望。观测数据是由确定的函数加一个随机噪声产生的
</p>
<p>
E\left[L\right]=&int;\left\{ y(\mathbf{x})-t\right\} <sup>2</sup>p(\mathbf{x},t)d\mathbf{x}dt=&int;\left\{ y(\mathbf{x})-h(\mathbf{x})\right\} <sup>2</sup>p(\mathbf{x})d\mathbf{x}+&int;\left\{ h(\mathbf{x})-t\right\} <sup>2</sup>p(\mathbf{\mathbf{x}},t)d\mathbf{x}dt
</p>
<p>
损失函数被分解成了二项，前一项表示的是真实值与回归函数之间的损失，后一项表示的是由于误差存在产生的损失。误差是存在的，后一项是不可消除的。也就是y(x,w)+&epsilon; 中的&epsilon; 这一项。
</p>
<p>
再对第一项进行分解
</p>
<p>
\left\{ y\left(\mathbf{x};D\right)-h\left(\mathbf{x}\right)\right\} <sup>2</sup>=\left\{ y\left(\mathbf{x};D\right)-E<sub>D</sub>[y\left(\mathbf{x};D\right)]+E<sub>D</sub>[y\left(\mathbf{x};D\right)]-h\left(\mathbf{x}\right)\right\} <sup>2</sup>=\left\{ y\left(\mathbf{x};D\right)-E<sub>D</sub>\left[y\left(\mathbf{x};D\right)\right]\right\} <sup>2</sup>+\left\{ E<sub>D</sub>\left[y\left(\mathbf{x};D\right)\right]-h\left(\mathbf{x}\right)\right\} <sup>2</sup>+2YH
</p>
<p>
于是得到E<sub>D</sub>\left[\left\{ y\left(\mathbf{x};D\right)-h\left(\mathbf{x}\right)\right\} <sup>2</sup>\right]=\left\{ E<sub>D</sub>\left[y\left(\mathbf{x};D\right)\right]-h\left(\mathbf{x}\right)\right\} <sup>2</sup>+E<sub>D</sub>\left[\left\{ y\left(\mathbf{x};D\right)-E<sub>D</sub>\left[y\left(\mathbf{x};D\right)\right]\right\} <sup>2</sup>\right]
</p>
<p>
第一项叫偏差，表示预测得到的平均值与与目标回归函数之前的差异。
</p>
<p>
第二项叫方差，表示单独各个数据集得到的结果与他们的均值之前的差异。
</p>
<p>
最小损失函数最终写成如下形式
</p>
<p>
expected\quad loss=(bias)<sup>2</sup>+variance+noice
</p>
<p>
其中\left(bias\right)<sup>2</sup>=&int;\left\{ E<sub>D</sub>\left[y\left(\mathbf{x};D\right)-h\left(\mathbf{x}\right)\right]\right\} <sup>2</sup>p\left(\mathbf{x}\right)d\mathbf{x}
</p>
<p>
variance=&int; E<sub>D</sub>\left[\left\{ y\left(\mathbf{x};D\right)-E<sub>D</sub>\left[y\left(\mathbf{x};D\right)\right]\right\} <sup>2</sup>\right]p\left(\mathbf{x}\right)d\mathbf{x}
</p>
<p>
noice=&int;\left\{ h\left(\mathbf{x}\right)-t\right\} <sup>2</sup>p\left(\mathbf{x},t\right)d\mathbf{x}dt
</p>
<p>
6 中心极限定理
</p>
<p>
n个独立同分布的随机变量求和，当n&rarr;&infin; 时，和的分布服从高斯分布
</p>
